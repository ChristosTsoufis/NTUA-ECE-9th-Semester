# -*- coding: utf-8 -*-
"""PatRec_Lab2_prep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/135tbLk4BEZ4vuYrVbh1TepDp34wcSF-1

## National Technical University of Athens
### School of Electrical & Computer Engineering
### Course: **Pattern Recognition**
##### *Flow S, 9th Semester, 2021-2022*

## Lab 2: Voice Recognition with Hidden Markovian Models (HMM) & Retroactive Neural Networks (RNN)

<br>

##### Full Name: Christos Tsoufis
##### A.M.: 031 17 176

### Installation of Packages
"""

!pip install gensim==3.8.1 matplotlib==3.1.0 nltk==3.4.4 numpy==1.16.4 pandas==0.24.2 pomegranate==0.12.0 scikit-image==0.15.0 scikit-learn==0.21.2 scipy==1.3.0 seaborn==0.9.0 torch==1.3.1 torchvision==0.4.2 tqdm==4.32.1 joblib==0.17.0
!pip install numba==0.48.0 --ignore-installed
!pip install librosa==0.7.1
!pip install word2number

"""### Imports & Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import os
from os import listdir
import re
import copy
import numpy as np
from numpy.core.numeric import NaN
import pandas as pd
import seaborn as sns
import math
import random
import IPython.display as ipd
from word2number import w2n

import librosa
import librosa.display

import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
import matplotlib.colors as mcolors
from mpl_toolkits.mplot3d import Axes3D

from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import zero_one_loss
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPClassifier as MLP
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.neighbors import KNeighborsClassifier
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import Normalizer
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import StandardScaler
# from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from mpl_toolkits.mplot3d import Axes3D

from glob import glob

import torch
from torch import nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torch.optim as optim

import pickle
from pomegranate import *
from scipy.stats import multivariate_normal
import itertools

import sys
import warnings
warnings.filterwarnings('ignore')

# %cd drive/My Drive/PatRec_Labs/Lab2

"""In order to run this notebook in Google Colab, a folder should be created in Google Drive with the name "PatRec_Labs". In this folder, another folder should be created, called "Lab2". Next, in this folder, folder "pr_lab2_2020-21_data" should be added from helios. Finally, from the menu on the left, choose "Files" and then "Prosartisi Drive" (here its in greek-lish) and re-run the cell.

The following cell is used to specify the correct location of the folders.
"""

cwd = os.getcwd()  # Get the current working directory (cwd)
files = os.listdir(cwd)  # Get all the files in that directory
print("Files in %r: %s" % (cwd, files))
print()

from google.colab import drive
drive.mount('/content/drive')

"""### Step 01: Sound Analysis using Praat

It was implemented using Praat.

### Step 02: Data Parser Function for wav files

The following cell defines some initial values.
"""

digit_dict = {"zero":0,"one":1,"two":2,"three":3,"four":4,
              "five":5,"six":6,"seven":7,"eight":8,"nine":9}
rev_digit_dict = {0:'zero',1:'one',2:'two',3:'three',4:'four',
                  5:'five',6:'six',7:'seven',8:'eight',9:'nine'}
Fs = 0 # Global variable that is set from parser()
samples = 133

"""The following cell implements the parser function."""

def parser(directory):
    global Fs
    # Parse relevant dataset info
    files = glob(os.path.join(directory, '*.wav'))
    fnames = [f.split('/')[3].split('.')[0] for f in files]
    matches = [re.match(r"([a-z]+)([0-9]+)", x, re.I) for x in fnames]
    ans = [x.groups() for x in matches]
    digits = [digit_dict[x[0]] for x in ans]
    speakers = [int(x[1]) for x in ans]
    _, Fs = librosa.core.load(files[0], sr=None)

    def read_wav(f):
        wav, _ = librosa.core.load(f, sr=None)
        return wav

    # Read all wavs
    wavs = [read_wav(f) for f in files]

    return wavs, speakers, digits

"""The following 2 cells run some examples in order to check that the implementation was correct."""

wavs, speakers, digits = parser("./pr_lab2_2020-21_data/digits/")

print("Printing Speakers:\n", speakers)
print()
print("Printing Digits:\n", digits)
print()

print("Printing some files again just to be sure:")
print()

for i in range(6):
    print("wavs:", i+1, ", speakers:", speakers[i], ", digits:", digits[i])
print()

sample = random.randint(0, samples-1)
wav_name = rev_digit_dict[digits[sample]].lower() + str(speakers[sample]) + '.wav'
print("A random wav file is", wav_name)

# listen check
ipd.Audio("./pr_lab2_2020-21_data/digits/" + wav_name)

"""### Step 03: MFCCs for each file and derivative calculation"""

# Fs = 0
sr = 16000
n_mfcc = 13

window_length = 0.025
n_fft = int (sr * window_length)

hop_length = 0.010
hop_length = int (sr * hop_length)

frames = [librosa.feature.mfcc(y=wav, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft).T for wav in wavs]
# frames = [librosa.feature.mfcc(y=wav, sr=Fs, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft).T for wav in wavs]

# Compute deltas and delta-deltas
mfcc_delta = [librosa.feature.delta(x) for x in frames]
mfcc_delta_delta = [librosa.feature.delta(x) for x in mfcc_delta]

# Standardize data
scaler = StandardScaler()
scaler.fit(np.concatenate(frames))
for i in range(len(frames)):
    frames[i] = scaler.transform(frames[i])

print("Printing dataset info (Total wavs):", len(frames))
print()

"""### Step 04: Depiction of MFCCs & MFSCs Histograms

The following cell is a draft implementation of some histograms for the 1st and 2nd MFCC of n1 = 7 & n2 = 6.
"""

wavs = np.array(wavs)
speakers = np.array(speakers)
digits = np.array(digits)

indexes = zip(np.where((digits == 'seven'))[0],
              np.where((digits == 'six'))[0])

mfcc_n1_0 = np.empty([2,2])
mfcc_n1_1 = np.empty([2,2])
mfcc_n2_0 = np.empty([2,2])
mfcc_n2_1 = np.empty([2,2])

for i, j in indexes:
  mfcc_n1_0 = np.append(mfcc_n1_0, frames[i][0])
  mfcc_n2_0 = np.append(mfcc_n2_0, frames[j][0])
  mfcc_n1_1 = np.append(mfcc_n1_1, frames[i][1])
  mfcc_n2_1 = np.append(mfcc_n2_1, frames[j][1])

"""Plotting"""

plt.hist(mfcc_n1_0, label= "n1 = 7 dimension 0")
plt.hist(mfcc_n2_0, label= "n2 = 6 dimension 0")
plt.title(label = "n1 = 7, n2 = 6 for all recordings in dimension at index 0")
plt.legend()
plt.show()

plt.hist(mfcc_n1_1, label = "n1 = 7 dimension 1")
plt.hist(mfcc_n2_1, label = "n2 = 6 dimension 1")
plt.title(label = "n1 = 7, n2 = 6 for all recordings in dimension at index 1")
plt.legend()
plt.show()

"""The following function will be used to make hist for MFCC."""

def make_hist(n,mfccs,flag):
    plt.hist(mfccs)
    plt.grid()
    plt.title('%s MFCC for digit %d' %(flag,n))
    plt.show()

"""The following function will be used to prep hist for MFCC. First, it finds the indices of all instances of digits n1,n2. Then, it applies the first coeff of all instances of n1,n2 and later the second coeff of all instances of n1,n2."""

def prep_hist(n1,n2,mfccs,digits):
    n1_index = [i for i, x in enumerate(digits) if x == n1]
    n2_index = [i for i, x in enumerate(digits) if x == n2]
    mfccs1_n1 = [list(mfccs[i][0]) for i in n1_index]
    mfccs1_n2 = [list(mfccs[i][0]) for i in n2_index]
    mfccs2_n1 = [list(mfccs[i][1]) for i in n1_index]
    mfccs2_n2 = [list(mfccs[i][1]) for i in n2_index]

    make_hist(n1,mfccs1_n1,"First")
    make_hist(n2,mfccs1_n2,"First")
    make_hist(n1,mfccs2_n1,"Second")
    make_hist(n2,mfccs2_n2,"Second")
    return n1_index,n2_index

n1_index, n2_index = prep_hist(7,6,frames,digits)

"""The following function is used to extract MFSC."""

def mfsc(n_index,wavs,n):
    mfsc_n = []
    for i in n_index[:2]:
        spec = librosa.feature.melspectrogram(wavs[i], n_mels=13)
        mfsc_n.append(spec)
        plt.figure()
        spec_db = librosa.power_to_db(spec,ref=np.max)
        librosa.display.specshow(spec_db, x_axis='time',
                             y_axis='mel',
                             sr=sr)
        plt.colorbar(format='%+2.0f dB')
        plt.title('Mel-frequency spectrogram for Digit %d' %(n))
        plt.tight_layout()
        plt.show()
    return mfsc_n

mfsc_n1 = mfsc(n1_index,wavs,7)
mfsc_n2 = mfsc(n2_index,wavs,6)

"""The following function is used for Correlation for MFCCs."""

def mfcc_corr(n1,n2,n1_index,n2_index,frames):
    fig, axes = plt.subplots(1,2,figsize = (12,12))
    cnt = 0
    for i in n1_index[:2]:
        ax = axes[cnt]
        cor = np.corrcoef(frames[i].T)
        bar = ax.matshow(cor)
        ax.set_title('Correlation for MFCCs digit %d' %(n1),fontsize = 10)
        ax.set_xlabel('MFCCs' ,fontsize = 12)
        ax.set_ylabel('MFCCs' ,fontsize = 12)
        cnt += 1

    fig.subplots_adjust(right=0.8)
    cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])
    fig.colorbar(bar, cax=cbar_ax)
    plt.show()

    fig, axes = plt.subplots(1,2,figsize = (12,12))
    cnt = 0
    for i in n2_index[:2]:
        ax = axes[cnt]
        cor = np.corrcoef(frames[i].T)
        bar = ax.matshow(cor)
        ax.set_title('Correlation for MFCCs digit %d' %(n2),fontsize = 10)
        ax.set_xlabel('MFCCs',fontsize = 12)
        ax.set_ylabel('MFCCs',fontsize = 12)
        cnt += 1
    fig.subplots_adjust(right=0.8)
    cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])
    fig.colorbar(bar, cax=cbar_ax)
    plt.show()

mfcc_corr(7,6,n1_index,n2_index,frames)

"""The following function is used for Correlation of MFSCs."""

def mfsc_corr(n1,n2,mfsc_n1,mfsc_n2):
    fig, axes = plt.subplots(1,2,figsize = (12,12))
    cnt = 0
    for i in mfsc_n1:
        ax = axes[cnt]
        cor = np.corrcoef(i.T)
        bar = ax.matshow(cor)
        ax.set_title('Correlation for MFSCs digit %d' %(n1),fontsize = 10)
        ax.set_xlabel('MFSCs' ,fontsize = 12)
        ax.set_ylabel('MFSCs' ,fontsize = 12)
        cnt += 1
    fig.subplots_adjust(right=0.8)
    cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])
    fig.colorbar(bar, cax=cbar_ax)
    plt.show()

    fig, axes = plt.subplots(1,2,figsize = (12,12))
    cnt = 0
    for i in mfsc_n2:
        ax = axes[cnt]
        cor = np.corrcoef(i.T)
        bar = ax.matshow(cor)
        ax.set_title('Correlation for MFSCs digit %d' %(n2),fontsize = 10)
        ax.set_xlabel('MFSCs',fontsize = 12)
        ax.set_ylabel('MFSCs',fontsize = 12)
        cnt += 1
    fig.subplots_adjust(right=0.8)
    cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])
    fig.colorbar(bar, cax=cbar_ax)
    plt.show()

mfsc_corr(7,6,mfsc_n1,mfsc_n2)

"""### Step 05: Depiction of the first approximation for the digit recognition & feature vector extraction

The following cell implements feature vector extraction. First, it calculates the mean value and the standard deviation for MFCCs, deltas and delta-deltas for all windows and then it horizontally concatenates them.
"""

n_mfcc = 13
features = np.zeros((len(frames), 6*n_mfcc)) 
for i in range(len(wavs)):
    features[i][0:n_mfcc]          = np.mean(frames[i], axis=0)
    features[i][n_mfcc:2*n_mfcc]   = np.mean(mfcc_delta[i], axis=0)
    features[i][2*n_mfcc:3*n_mfcc] = np.mean(mfcc_delta_delta[i], axis=0)
    features[i][3*n_mfcc:4*n_mfcc] = np.std(frames[i], axis=0)
    features[i][4*n_mfcc:5*n_mfcc] = np.std(mfcc_delta[i], axis=0)
    features[i][5*n_mfcc:6*n_mfcc] = np.std(mfcc_delta_delta[i], axis=0)

groups_x = [[] for i in range(9)]
groups_y = [[] for i in range(9)]
for i in range(len(wavs)):
    groups_x[digits[i]-1].append(features[i][0:n_mfcc][0])
    groups_y[digits[i]-1].append(features[i][3*n_mfcc:4*n_mfcc][0])

colors = ['gray','lightcoral','darkred','peru','lightgreen', 'darkgreen', 'turquoise', 'blue', 'gold']

plt.figure(figsize = (10,10))

for i in range(9):
    plt.scatter(groups_x[i], groups_y[i], label = f'{i+1}', color = colors[i], marker='o')

plt.legend(fontsize = 12)
plt.xlabel('Mean MFCC first Coefficient')
plt.ylabel('Mean Delta first Coefficient')
plt.title('2-D Mean Signals Representation')
plt.grid()
plt.show()

def scattering(X, y, dim):
    plt.rcParams['figure.figsize'] = [10, 10]
    fig, ax = plt.subplots()
    X0 = X[:, 0]
    X1 = X[:, 1]
    for i in range(9):
      ax.scatter(X0[y==i+1], X1[y==i+1], color=colors[i], marker='o', label=f'{i+1}')
    ax.set_xticks(())
    ax.set_yticks(())
    ax.legend()
    plt.xlabel('Standard deviation MFCC first Coefficient')
    plt.ylabel('Standard deviation Delta first Coefficient')
    plt.title('2-D Ïƒ Signals Representation')
    plt.show()

scattering(features[:,39:], digits, 2)

"""The following cell is another implementation of feature vector extraction."""

new_vectors = []
new_vectors_ex = []

for i in range(len(frames)):
  connection = np.concatenate([frames[i],mfcc_delta[i],mfcc_delta_delta[i]], axis=0)
  vector = np.concatenate((connection.mean(axis=1),connection.std(axis=1)))
  new_vectors.append(vector)

print("Length of vectors: ", len(new_vectors))
print("Unique Digits: ", np.unique(digits))
print("Length of new Extended vectors: ", len(new_vectors))
markers = list(Line2D.markers.keys())[0:9]
colors = list(mcolors.TABLEAU_COLORS.keys())[0:9]

iter = 0
fig, ax = plt.subplots(figsize=(10,10))

for digit in np.unique(digits):
  indices = np.where((digits==digit))[0]
  flag = True
  for i in indices:
    vector = new_vectors[i]
    if flag:
      ax.scatter(vector[0],vector[1],marker=markers[iter],color=colors[iter],label=digit)
      flag = False
    else:
      ax.scatter(vector[0],vector[1],marker=markers[iter],color=colors[iter])
  iter = iter + 1
ax.legend(loc="best")
ax.grid(True)
plt.show()

"""### Step 06: Depiction of the approximation with PCA for the digit recognition

The following cell implements the Principal Component Analysis in 2D.
"""

pca = PCA(n_components=2)
principalComps = pca.fit_transform(features)
groups_x = [[] for i in range(9)]
groups_y = [[] for i in range(9)]

for i in range(len(features)):
    groups_x[digits[i]-1].append(principalComps[i,0])
    groups_y[digits[i]-1].append(principalComps[i,1])

fig = plt.figure(figsize = (10,10))
ax = fig.add_subplot(1,1,1)
ax.set_xlabel('Principal Component 1', fontsize = 12)
ax.set_ylabel('Principal Component 2', fontsize = 12)
ax.set_title('2-D Mean Signals Representation', fontsize = 18)
for i in range(9):
    ax.scatter(groups_x[i], groups_y[i], label = f'{i+1}', color = colors[i])
ax.legend(fontsize=12)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
ax.grid()
plt.show()
print("PCA 2 variance ratio: ",pca.explained_variance_ratio_)

pca2D_var  = PCA(n_components=2)
feature_vector_2D_var  = pca2D_var.fit_transform(features[:,:39])
print(pca2D_var.explained_variance_ratio_)

scattering(feature_vector_2D_var, digits, 2)

"""The following cell implements the Principal Component Analysis in 3D."""

pca = PCA(n_components=3)

principalComps = pca.fit_transform(features)
groups_x = [[] for i in range(9)]
groups_y = [[] for i in range(9)]
groups_z = [[] for i in range(9)]

for i in range(len(features)):
    groups_x[digits[i]-1].append(principalComps[i,0])
    groups_y[digits[i]-1].append(principalComps[i,1])
    groups_z[digits[i]-1].append(principalComps[i,2])


fig = plt.figure(figsize = (10,10))
ax = fig.gca(projection='3d')

ax.set_xlabel('Principal Component 1', fontsize = 12)
ax.set_ylabel('Principal Component 2', fontsize = 12)
ax.set_zlabel('Principal Component 3', fontsize = 12)
ax.set_title('3 component PCA', fontsize = 18)
for i in range(9):
    ax.scatter3D(groups_x[i], groups_y[i], groups_z[i], label = f'{i+1}', color = colors[i])
ax.legend(fontsize=12)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
ax.grid()
plt.show()
print("PCA 3 variance ratio: ",pca.explained_variance_ratio_)

"""### Step 07: Classifiers

This is the **1st** implementation.
"""

X = copy.deepcopy(features)
y = copy.deepcopy(digits)

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

classifiers = [
('Gaussian Naive Bayes', GaussianNB()),
('k-Nearest Neighbors, k = 1', KNeighborsClassifier(5)),
('2-Layer Perceptron', MLP(hidden_layer_sizes = (512), max_iter = 10000, alpha = 1)),
('Linear SVM', SVC(kernel = 'linear', C=0.01))
]

predictions = [clf.fit(X_train, y_train).predict(X_test) for _, clf in classifiers]

for i, y_pred in enumerate(predictions):
    print(f'Accuracy of {classifiers[i][0]}: {1 - zero_one_loss(y_test, y_pred)}')

""">Comment: I saw that by using the recommended package "scikit-learn==0.21.2" I could not import "from sklearn.metrics import plot_confusion_matrix" and I found this link (https://stackoverflow.com/questions/59169403/sklearn-wont-properly-import-plot-confusion-matrix) and this link (https://stackoverflow.com/questions/63967530/importerror-cannot-import-name-plot-confusion-matrix-from-sklearn-metrics) that suggest to change the version to 0.22.0 and then the problem will be fixed.

The following code, with the appropriate packets prints the Classifiers. It was run in a different environment and the results can be seen in the report.
```
from sklearn.metrics import plot_confusion_matrix

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))

for (name, cls), ax in zip(classifiers, axes.flatten()):
    plot_confusion_matrix(cls,
                          X_test,
                          y_test,
                          ax=ax,
                          cmap='Blues')
    ax.title.set_text(name)
plt.tight_layout()
plt.show()
```

This is the **2nd** implementation.

This function calculates the a-priori probabilities for every class and returns a np.ndarray with n_classes of the prior probabilities for every class

X is a np.ndarray with Digits data (nsamples * nfeatures)

y is also a np.ndarray with Labels for dataset (nsamples)
"""

def calculate_priors(X, y):
    apriori = []
    for label in np.unique(y):
        apriori.append(len(X[y==label])/len(X[:, 0]))
    return apriori

"""This function keeps all the corresponding samples in the list of the appropriate index, for each label so that it is easy to fetch any number of samples that belong to a certain label."""

def getLabelLines(y):
    label_lines_X = [[] for _ in range(10)]
    for i in range(0, y.size):
        (label_lines_X[int(y[i])]).append(i)
    return label_lines_X

"""This class implements a Custom Naive Bayes classifier."""

class CustomNBClassifier(BaseEstimator, ClassifierMixin):
    
    # this function is used for the initialization

    def __init__(self, use_unit_variance=False):
        self.X_mean_ = None
        self.use_unit_variance = use_unit_variance
        self.X_variance_ = None
        self.Y_priors_ = None
    
    # this function implements the fit of the classifier
    # it calculates self.X_mean_, self.X_variance_ and self.Y_priors_ based on
    # the mean feature values in X for each class and self.X_mean_ becomes 
    # a numpy.ndarray of shape (n_classes, n_features) and it returns self

    def fit(self, X, y):
        n_rows, n_cols = X.shape
        c_rows = len(np.unique(y))

        means_all_labels = np.empty((c_rows,n_cols))
        vars_all_labels = np.empty((c_rows,n_cols))

        label_lines_X = [[np.where(y == label)[0]] for label in np.unique(y)]

        for i in range(c_rows):
            for j in range(n_cols):
                means_all_labels[i,j] = np.mean(X[label_lines_X[i], j])
                vars_all_labels[i,j]  = np.var(X[label_lines_X[i], j])

        # replace zeros with small value for variances
        vars_all_labels[vars_all_labels == 0] = 10e-3

        self.X_mean_ = means_all_labels
        self.X_variance_ = vars_all_labels

        if(self.use_unit_variance):
          self.X_variance_[self.X_variance_ != 1] = 1
        
        self.Y_priors_ = calculate_priors(X,y)

        return self
    
    # this function makes predictions for X based on the euclidean 
    # distance from self.X_mean_

    def predict(self, X):
        n_rows, _ = X.shape
        Py = np.zeros((n_rows,len(self.Y_priors_)))

        for i in range (len(self.Y_priors_)):
          cov = self.X_variance_[i]*np.eye(len(self.X_variance_[i]))
          Py[:,i] = multivariate_normal.logpdf(X, mean=self.X_mean_[i], cov=self.X_variance_[i]) + np.log(self.Y_priors_[i])
        
        y_pred = (np.argmax(Py,axis=1)) + 1

        return y_pred

    # this function returns the accuracy score on the predictions for
    # X based on ground truth y

    def score(self, X, y):

        return accuracy_score(self.predict(X),y)

    # this function makes predictions for the probabilities

    def predict_proba(self,X):
        n_rows, _ = X.shape
        Py = np.zeros((n_rows,len(self.Y_priors_)))
        
        for i in range (len(self.Y_priors_)):
            Py[:,i] = np.exp(multivariate_normal.logpdf(X, mean=self.X_mean_[i], cov=self.X_variance_[i]) + np.log(self.Y_priors_[i]))
            
        return Py

scaler = Normalizer().fit(X_train)

X_train_normalized = scaler.transform(X_train)
X_test_normalized = scaler.transform(X_test)

def perform_test_classifications(X_train,y_train,X_test,y_test):
  # GNB Classifier
  clf = GaussianNB()
  clf.fit(X_train_normalized,y_train)
  y_predict = clf.predict(X_test_normalized)
  score = accuracy_score(y_predict,y_test)
  print("Accuracy of sklearn NB: ", score)

  my_nb = CustomNBClassifier()

  my_nb.fit(X_train_normalized, y_train)

  score = my_nb.score(X_test_normalized, y_test)
  print("Accuracy of custom NB: ", score)

  # 4 more classifiers
  from sklearn.neural_network import MLPClassifier as MLP
  clf_MLP = MLP()
  clf_MLP.fit(X_train_normalized, y_train)
  yMLP = clf.predict(X_test_normalized)
  print("Score of 2-Layer Perceptron (MLP) is: ", accuracy_score(yMLP, y_test))

  from sklearn.ensemble import RandomForestClassifier

  clf_RFC = RandomForestClassifier()
  clf_RFC.fit(X_train_normalized, y_train)
  yRFC = clf.predict(X_test_normalized)
  print("Score of Random Forest Classifier is: ", accuracy_score(yRFC, y_test))

  from sklearn import svm

  clf_svm = svm.SVC(gamma='scale',kernel='rbf')
  clf_svm.fit(X_train_normalized, y_train)
  ySVM=clf_svm.predict(X_test_normalized)
  print("Score of SVM classifier: ", accuracy_score(ySVM, y_test))

  from sklearn.neighbors import KNeighborsClassifier

  clf_2nn = KNeighborsClassifier(n_neighbors=2)
  clf_2nn.fit(X_train_normalized, y_train)
  yKNN = clf_2nn.predict(X_test_normalized)
  print("Score of KNN (n=2) classifier: ", accuracy_score(yKNN, y_test))

  return

print("Classification Results for simple features vectors")
perform_test_classifications(X_train, y_train, X_test, y_test)

"""(Bonus)

The implementation of the bonus can be seen in the older code.

### Step 08: PyTorch - RNN

This is the **1st** implementation.
"""

torch.manual_seed(5) # reproducible
# Hyper Parameters
length = 10

sin_values = []
cos_values = []
points_values = []
f = 40
for dist in range(300): # generate sequences of sines and cosines with length 10
  start, end = dist * np.pi, (dist+1)*np.pi 
  seq = np.linspace(start, end, length, dtype=np.float32)
  s_np = np.sin(2*np.pi*f *seq) 
  c_np = np.cos(2*np.pi*f*seq)

  # convert sequences as tensor sequences
  sin_values.append(torch.from_numpy(s_np[np.newaxis, :, np.newaxis]))
  cos_values.append(torch.from_numpy(c_np[np.newaxis, :, np.newaxis])) 
  points_values.append(seq)

class Model(nn.Module):
 def __init__(self, net,hidden_size=36,input_size=1):
    super(Model, self).__init__()
    self.net = net
    if net == 'rnn':
      self.network = nn.RNN(
      input_size=input_size,
      hidden_size=hidden_size, # rnn hidden unit
      num_layers=1, # number of rnn layer
      batch_first=True, # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)
      )
    elif net == 'lstm':
      self.network = nn.LSTM(
      input_size=input_size,
      hidden_size=hidden_size, # lstm hidden unit
      num_layers=1, # number of lstm layer
      batch_first=True, 
      )
    else:
      self.network = nn.GRU(
      input_size=input_size,
      hidden_size=hidden_size, # gru hidden unit
      num_layers=1, # number of gru layer
      batch_first=True, 
      )
    self.out = nn.Linear(hidden_size, 1)
 def forward(self, x, h_state):
    r_out, h_state = self.network(x, h_state)
    outs = [] # save all predictions
    for time_step in range(r_out.size(1)): # calculate output for each time step
      outs.append(self.out(r_out[:, time_step, :]))
    return torch.stack(outs, dim=1), h_state

net_attr = 'gru'
net = Model(net_attr)
lr = 0.02 # learning rate

optimizer = torch.optim.Adam(net.parameters(), lr=lr) # optimize
loss_func = nn.MSELoss()
h_state = None # for initial hidden state
c0 = None
iter = 0
plt.figure(figsize=(12, 5))
predicts = []
real=[]
for X,Y in zip(sin_values,cos_values):
  
  if net_attr != 'lstm':
    prediction, h_state = net(X, h_state) # rnn gru output take only hidden state in forward
    h_state = h_state.data
  else:
    if h_state is None and c0 is None:
      prediction, values = net(X, None) # initial for lstm 
    else:
      prediction, values = net(X, (h_state,c0)) # lstm 
    h_state = values[0].data
    c0 = values[1].data
  loss = loss_func(prediction, Y) # cross entropy loss
  optimizer.zero_grad() # clear gradients for this training step
  loss.backward()

  optimizer.step() 
  predicts.append(prediction.data.numpy().flatten())
  real.append(Y.data.numpy().flatten())
  iter +=1

#plot real cosine sequence with predicted(blue)
plt.title(net_attr.upper())
plt.plot(np.array(points_values).flatten(), np.array(real).flatten(), 'r-',label="real cos")
plt.plot(np.array(points_values).flatten(),np.array(predicts).flatten(), 'b-',label="predicted cos")
plt.legend()
plt.show()

net_attr = 'rnn'
net = Model(net_attr)
lr = 0.02 # learning rate

optimizer = torch.optim.Adam(net.parameters(), lr=lr) # optimize
loss_func = nn.MSELoss()
h_state = None # for initial hidden state
c0 = None
iter = 0
plt.figure(figsize=(12, 5))
predicts = []
real=[]
for X,Y in zip(sin_values,cos_values):
  
  if net_attr != 'lstm':
    prediction, h_state = net(X, h_state) # rnn gru output take only hidden state in forward
    h_state = h_state.data
  else:
    if h_state is None and c0 is None:
      prediction, values = net(X, None) # initial for lstm 
    else:
      prediction, values = net(X, (h_state,c0)) # lstm 
    h_state = values[0].data
    c0 = values[1].data
  loss = loss_func(prediction, Y) # cross entropy loss
  optimizer.zero_grad() # clear gradients for this training step
  loss.backward()

  optimizer.step() 
  predicts.append(prediction.data.numpy().flatten())
  real.append(Y.data.numpy().flatten())
  iter +=1

#plot real cosine sequence with predicted(blue)
plt.title(net_attr.upper())
plt.plot(np.array(points_values).flatten(), np.array(real).flatten(), 'r-',label="real cos")
plt.plot(np.array(points_values).flatten(),np.array(predicts).flatten(), 'b-',label="predicted cos")
plt.legend()
plt.show()

net_attr = 'lstm'
net = Model(net_attr)
lr = 0.02 # learning rate

optimizer = torch.optim.Adam(net.parameters(), lr=lr) # optimize
loss_func = nn.MSELoss()
h_state = None # for initial hidden state
c0 = None
iter = 0
plt.figure(figsize=(12, 5))
predicts = []
real=[]
for X,Y in zip(sin_values,cos_values):
  
  if net_attr != 'lstm':
    prediction, h_state = net(X, h_state) # rnn gru output take only hidden state in forward
    h_state = h_state.data
  else:
    if h_state is None and c0 is None:
      prediction, values = net(X, None) # initial for lstm 
    else:
      prediction, values = net(X, (h_state,c0)) # lstm 
    h_state = values[0].data
    c0 = values[1].data
  loss = loss_func(prediction, Y) # cross entropy loss
  optimizer.zero_grad() # clear gradients for this training step
  loss.backward()

  optimizer.step() 
  predicts.append(prediction.data.numpy().flatten())
  real.append(Y.data.numpy().flatten())
  iter +=1

#plot real cosine sequence with predicted(blue)
plt.title(net_attr.upper())
plt.plot(np.array(points_values).flatten(), np.array(real).flatten(), 'r-',label="real cos")
plt.plot(np.array(points_values).flatten(),np.array(predicts).flatten(), 'b-',label="predicted cos")
plt.legend()
plt.show()

"""This is the **2nd** implementation."""

import math
import copy
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
from   torch.utils.data import Dataset, DataLoader

def generate_data():
    f = 40
    T = 1/f
    w = 2 * math.pi * f
    xs = np.linspace(0, T, 10) # 10 samples in one period

    phase = np.random.uniform(0, 2*np.pi, N_SAMPLES) # produce a random phase

    sin_waves = [np.sin(w * xs + phase[idx]) for idx in range(N_SAMPLES)]
    cos_waves = [np.cos(w * xs + phase[idx]) for idx in range(N_SAMPLES)]

    sin_as_torch  = torch.tensor(sin_waves).float().unsqueeze(dim = -1)
    cos_as_torch  = torch.tensor(cos_waves).float()

    return sin_as_torch, cos_as_torch

class WavesDataset(Dataset):
    def __init__(self, sines, cosines, trans = None):
        self.data = list(zip(sines, cosines))
        self.trans = trans

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        if self.trans is not None:
            return self.trans(self.data[idx])
        else:
            return self.data[idx]

class RNNEncoderDecoder(nn.Module):
    def __init__(self):
        super(RNNEncoderDecoder, self).__init__()
        self.encoder = nn.RNN(1, 256, batch_first = True)
        self.decoder = nn.RNN(1, 256, batch_first = True)
        self.linear  = nn.Linear(256,1)

    def forward(self, x):
        batch_size = x.size(0)

        # RNN Encoder consumes the input one by one
        # and we keep its last state
        # hopefully this encodes sine's amplitude and phase
        last_hidden_state = self.encoder(x)[1]

        predictions = [torch.zeros(batch_size, 1, 1)]
        for _ in range(10):
            # Sequantially produce the outputs with the decoder RNN
            # Translate the outputs from 1x256 to 1x1 with the Linear layer
            zero = torch.zeros(batch_size, 1, 1)
            last_hidden_state = self.decoder(zero, last_hidden_state)[1]
            prediction = self.linear(last_hidden_state[0]).unsqueeze(dim = -1)
            predictions.append(prediction)


        # drop the initial zero in predictions
        s = torch.stack(predictions[1:]).squeeze(dim = -1).squeeze(dim = -1)
        batch_result = torch.transpose(s, 0, 1)

        return batch_result

class SimpleRNN(nn.Module):
    def __init__(self, hidden_size):
        super(SimpleRNN, self).__init__()
        self.rnn         = nn.RNN(1, hidden_size, batch_first = True)
        self.linear      = nn.Linear(hidden_size, 1)
        self.hidden_size = hidden_size

    def forward(self, x):
        # x -> batch x seq x feature -> BATCH_SIZE x 10 x 1
        batch_size = x.size(0)
        seq_length = x.size(1)
        outputs, _ = self.rnn(x)

        predicted_batch = torch.zeros(batch_size, seq_length, 1)
        for i in range(batch_size):
            output = outputs[i] # output -> 10 x 256
            predicted_batch[i] = self.linear(output)

        return predicted_batch.squeeze(dim = -1)

def train(model, learning_rate, momentum):
    X, y         = generate_data()
    train_data   = WavesDataset(X,y)
    train_loader = DataLoader(train_data, batch_size = BATCH_SIZE)
    loss         = nn.MSELoss(reduction = 'mean')
    optimizer    = optim.SGD(model.parameters(),
                             lr = learning_rate,
                             momentum = momentum
                            )
    best_loss  = None
    best_model = None
    print("==========================================")

    for epoch in range(EPOCHS):
        total_loss = 0
        for i, data in enumerate(train_loader):
            X_batch, y_batch = data
            y_pred = model(X_batch)
            L      = loss(y_pred, y_batch)
            total_loss += L.item()

            optimizer.zero_grad()
            L.backward()
            optimizer.step()

        avg_loss = total_loss / BATCH_SIZE
        print(f'Epoch {epoch}, loss {avg_loss:.6f}')

        if( best_loss is None or avg_loss < best_loss ):
            best_loss = avg_loss
            best_model = copy.deepcopy(model.state_dict())

    print("==========================================")
    return best_model

def plot_test(title, prediction, actual):
    plt.figure(figsize=(10,8))
    plt.title(title)
    plt.plot(xs, prediction.numpy(), 'o-', label = 'Predicted wave')
    plt.plot(xs, actual, 'o-.', label = 'Actual Wave')
    plt.legend()
    plt.grid()
    plt.show()

# create data
f = 40
T = 1/f
w = 2*np.pi*f
n_seq = 1000   # number of sinus sequences
n_points = 10  # sample points

sin_waves = np.zeros((n_seq, n_points))
cos_waves = np.zeros((n_seq, n_points))

for i in range(n_seq):
    
    A = np.random.uniform(1,10)
    start = np.random.uniform(0, T)
    final = start + n_points*0.001  # bring the points close together
    t = np.linspace(start, final, n_points)
    
    sin_waves[i] = A*np.sin(w*t)
    cos_waves[i] = A*np.cos(w*t)

# we plot some of our generated data
fig = plt.figure(figsize=(20,10))
for i in range(5):  # sinus
    fig.add_subplot(2, 5, i+1)
    plt.plot(np.arange(10), sin_waves[i], color='blue')
plt.show()

fig = plt.figure(figsize=(20,10))
for i in range(5): # cosine
    fig.add_subplot(2, 5, i+6)
    plt.plot(np.arange(10), cos_waves[i], color='green')
plt.show()

"""The following cell takes approx. 2'."""

N_SAMPLES = 5000
BATCH_SIZE = 512
EPOCHS = 30

m1 = SimpleRNN(1)
m2 = SimpleRNN(32)
m3 = RNNEncoderDecoder()

best_model1 = train(m1, 1e-2, 0.73)
best_model2 = train(m2, 1e-2, 0.73)
best_model3 = train(m3, 1e-2, 0.73)

# Visualize results
f = 40
T = 1/f
w = 2 * math.pi * f
xs = np.linspace(0, T, 10)
test_phase = 42 * np.pi / 17
test_sin = np.sin(w * xs + test_phase)
test_cos = np.cos(w * xs + test_phase)

m1.load_state_dict(best_model1)
m2.load_state_dict(best_model2)
m3.load_state_dict(best_model3)

with torch.no_grad():
    prediction1 = m1(torch.tensor(test_sin).float().unsqueeze(-1).unsqueeze(0))[0]
    prediction2 = m2(torch.tensor(test_sin).float().unsqueeze(-1).unsqueeze(0))[0]
    prediction3 = m3(torch.tensor(test_sin).float().unsqueeze(-1).unsqueeze(0))[0]

plot_test('Simple RNN with hidden state = 1', prediction1, test_cos)

plot_test('Simple RNN with hidden state = 32', prediction2, test_cos)

plot_test('RNN Encoder/Decoder', prediction3, test_cos)